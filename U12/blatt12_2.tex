\documentclass[12pt]{article}
\usepackage[a4paper,left=10mm,right=10mm,top=15mm,bottom=15mm]{geometry}
\usepackage{amssymb,amsthm,latexsym,amsfonts, amsmath, bm}
\usepackage[lined,ruled]{algorithm2e}
\usepackage{extarrows}
\usepackage{enumerate}
\usepackage{tikz}
\usepackage{underoverlap}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{float}
\usepackage{array}
\newtheorem*{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\title{Exercises for Algorithmic Bioinformatics II\\
Assignment 11}
\author{Xiheng He}
\date{Januar 2022}
\linespread{2.0}
\begin{document}
\flushright{Wintersemester 2021/22}
\flushleft{Xiheng He}
\flushleft{Lisanne Friedrich}
{\let\newpage\relax\maketitle}
\begin{flushleft}
\textbf{Exercise 2 (Entropy, 10P):}
\newline
Given the following joint distribution of two random variables X and Y :
\newline
\begin{tabular}{c|cc}
    & Y = 0 & Y = 1 \\
    \hline
    X = 0 & $\frac{1}{3}$ & $\frac{1}{3}$ \\
    X = 1 & 0 & $\frac{1}{3}$
\end{tabular}
\newline \\
and entropy defined as $H(Y|X) := \sum_x P(x)H(Y|X = x)$ and $H(X,Y) := -\sum_{x,y} P(x,y)\log P(x,y)$.
Calculate the entropies, relative entropies and mutual information:
\begin{enumerate}[(a)]
    \item $H(X)$, $H(Y)$
    \begin{align*}
        & H(X) = -\sum_{i} P(x_i) \log P(x_i) \\
        & = -(1/3 \cdot \log(1/3) + (1 - 1/3) \cdot \log (1 - 1/3)) \\
        & = \log(3) - \displaystyle \frac{2}{3} \\
        & H(Y) = -\sum_{i} P(y_i) \log P(y_i) \\
        & = -(1/3 \cdot \log(1/3) + (1 - 1/3) \cdot \log (1 - 1/3)) \\
        & = \log(3) - \displaystyle \frac{2}{3} \\
    \end{align*}
    \item $H(X|Y)$, $H(Y|X)$
    \begin{align*}
        & H(X|Y) = H(Y|X) = \sum_{x_i} P(x_i) H(Y | X = x_i) \\
        & = 0 \times 1/3 + 1 \times 2/3 = \displaystyle \frac{2}{3}
    \end{align*}
    \item $H(X,Y)$
    \begin{align*}
        & H(X,Y) = -\sum_{x,y} P(x,y) \log P(x,y) \\
        & = - (1/3 \cdot \log (1/3) + 1/3 \cdot \log(1/3) + 1/3 \cdot \log(1/3)) = \log 3
    \end{align*}
    \item $H(X || Y)$
    \begin{align*}
        & H(X || Y) = \sum_i P(x_i) \log(\frac{P_X(x_i)}{P_Y(x_i)}) \\
        & = 2/3 \times \log(2/3 \div 1/3) + 1/3 \times \log(1/3 \div 2/3) = 1/3
    \end{align*}
    \item $M(X;Y)$
    \begin{align*}
        & M(X;Y) = \sum_{i,j} P(x_i, y_j) \log(\frac{P(x_i,y_j)}{P(x_i) P(y_j)}) \\
        & = log(3) - 3/4
    \end{align*}
\end{enumerate}
\end{flushleft}
\end{document}